---
title: "Prediction in Exercise Pattern behaviours"
author: "Wanda Ken - Date: 16/04/2021"
geometry: "left=.5cm,right=.5 cm,top=.6cm,bottom=1.1cm"
output:
  html_document: 
    keep_md: yes
  pdf_document: 
    fig_crop: no
    toc_depth: 1
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
Data from accelerometers on the belt, forearm, arm and dumbell of 6 participants will be used to predict the manner in which they did the exercise.  The "classe" variable will be used to determine this.  The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

More information is available in the section on the Weight Lifting Exercise Dataset from the link:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

We will build a model from the Training set and use the validation set to predict "classe".  The prediction methods that will be used are **Decision Trees, Gradient Boosting and Random Forest**.  The accuracy and out of Sample Error will be calculated and compared for each model.

The prediction model with the best accuracy and hence lowest error will be used to predict "classe" values for the testing set of 20 observations provided in the link.

## Loading the training data
```{r TrainSet, echo=TRUE}
url <- "C:/Users/gwan/Desktop/pml-training.csv"
training = read.csv(url)

## get the columns/rows available
dim(training)

## Sample Structure of the dataset
str(training[,1:10])
```

## Loading the testing data
```{r TestSet, echo=TRUE}
url <- "C:/Users/gwan/Desktop/pml-testing.csv"
testingfinal = read.csv(url)

## get the rows/columns available
dim(testingfinal)

## Sample Structure of the dataset
str(testingfinal[,1:10])
```
This finaltesting dataset will be used for out final prediction of "classe".

## Loading the packages 
```{r Pack, echo=TRUE}
library(ggplot2)
library(lattice)
library(caret)
library(randomForest)
library(rattle)

## set the seed data
set.seed(300)

```

## Cleaning data
```{r Clean, echo=TRUE}

## Remove the first 7 columns which are not relevant to the study
training <- training[,-c(1:7)]

## Remove variables that have little variability
nvz <- nearZeroVar(training)
training <- training[,-nvz]

## Remove variables with NAS
training <- training[,colSums(is.na(training))==0]

dim(training)

```
The training dataset have now 53 columns


## Splitting the training set
The training set is divided in two parts one for training and the other for cross validation.
70% is used as training set and 30% as the validation set.

```{r split, echo=TRUE}
trainInd <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
train <- training[trainInd,]
test  <- training[-trainInd,]
dim(train)
dim(test)

## create a train control variable with sample method of cross validation and number of folds = 3
control <- trainControl(method="cv", number=3, verboseIter=FALSE)

```

# Building and Testing the Models

## 1. Decision Tree
```{r decisiontree, echo=TRUE,fig.height=4}

fitdt <- train(classe ~ .,data=train,method="rpart")
preddt <- predict(fitdt,test)

# need to covert to factor as test$classe is chr
cfm <- confusionMatrix(preddt, as.factor(test$classe))

cfm

fancyRpartPlot(fitdt$finalModel)
```

## 2. Gradient Boosting 
```{r boost, echo=TRUE}
fitbt <- train(classe ~ ., method="gbm", data=train, trControl=control, tuneLength = 5,verbose=FALSE)
predbt <- predict(fitbt,test)
# need to convert to factor as test$classe is chr
cfm <- confusionMatrix(predbt, as.factor(test$classe))
cfm
#plot(fitbt)

```


## 3. Random Forest
```{r randomforest, echo=TRUE}
fitrf <- train(classe~ .,data=train, method="rf", trControl=control, tuneLength = 5)
predrf <- predict(fitrf,test)
# need to covert to factor as test$classe is chr
cfm <- confusionMatrix(predrf, as.factor(test$classe))
cfm
plot(fitrf,main="Figure 1: Random Forest : Mean Decrease Accuracy ")
```

```{r randomforest2, echo=TRUE}
v <- randomForest(formula = as.factor(classe) ~ . , data = train, ntree=100,mtry=2, importance = TRUE)
varImpPlot(v, main = "Figure 2: Random Forest : Measurement of variable Importance")

plot(v,main="Figure 3: Random Forest : Error rate v/s No of Trees")
```

- Figure 1: The model accuracy plot shows that maximum accuracy is achieved between 10-20 predictors.
- Figure 2: The Mean Decrease Accuracy plot shows that "roll_belt" is the most important variable.
- Figure 3: The Error Rate decreases as the number of trees increases.

## Comparing the Models
- The accuracy for Decision Tree is 0.4929 and Out of Sample Error is 0.5071
- The accuracy for Gradient Boosting is 0.9893 and Out of Sample Error is 0.0107
- The accuracy for Random Forest is 0.9964 and Out of Sample Error is 0.0036
- As the Random Forest has the best accuracy and lowest Out of Sample error among the model fits, we will apply this model of prediction on the Testing set.

## Prediction on Final Testing data using best model 
```{r finalresult, echo=TRUE}
pred <- predict(fitrf,newdata=testingfinal)
pred
```
The above predictions will be used for the project quiz.
